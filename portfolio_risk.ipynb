{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265c91a2-d8fa-4eac-b4c0-5af40d3418d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: portfolio_risk.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "\n",
    "from data_loader import fetch_monthly_close\n",
    "from factor_utils import (\n",
    "    calc_monthly_returns,\n",
    "    fetch_excess_return,\n",
    "    fetch_peer_median_monthly_returns,\n",
    "    compute_stock_factor_betas,\n",
    ")\n",
    "\n",
    "\n",
    "def normalize_weights(weights: Dict[str, float], normalize: bool = True) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Ensure weights sum to 1. If normalize is False, returns as-is.\n",
    "    \"\"\"\n",
    "    if not normalize:\n",
    "        return weights\n",
    "    total = sum(weights.values())\n",
    "    if total == 0:\n",
    "        raise ValueError(\"Sum of weights is zero, cannot normalize.\")\n",
    "    return {t: w / total for t, w in weights.items()}\n",
    "\n",
    "def compute_portfolio_returns(\n",
    "    returns: pd.DataFrame,\n",
    "    weights: Dict[str, float]\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Given a DataFrame of individual asset returns (columns = tickers)\n",
    "    and a dict of weights, compute the weighted portfolio return series.\n",
    "    \"\"\"\n",
    "    w = normalize_weights(weights)\n",
    "    # align columns and weights\n",
    "    aligned = returns[list(w.keys())].dropna()\n",
    "    weight_vec = np.array([w[t] for t in aligned.columns])\n",
    "    # dot product row-wise\n",
    "    port_ret = aligned.values.dot(weight_vec)\n",
    "    return pd.Series(port_ret, index=aligned.index, name=\"portfolio\")\n",
    "\n",
    "def compute_covariance_matrix(\n",
    "    returns: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute the sample covariance matrix of asset returns.\n",
    "    \"\"\"\n",
    "    return returns.cov()\n",
    "\n",
    "def compute_correlation_matrix(\n",
    "    returns: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute the sample correlation matrix of asset returns.\n",
    "\n",
    "    Args:\n",
    "        returns (pd.DataFrame): DataFrame where each column is an asset's return series.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Correlation matrix between assets.\n",
    "    \"\"\"\n",
    "    return returns.corr()\n",
    "\n",
    "def compute_portfolio_volatility(\n",
    "    weights: Dict[str, float],\n",
    "    cov_matrix: pd.DataFrame\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute portfolio volatility = sqrt(w^T Σ w).\n",
    "    \"\"\"\n",
    "    w = normalize_weights(weights)\n",
    "    w_vec = np.array([w[t] for t in cov_matrix.index])\n",
    "    var_p = float(w_vec.T.dot(cov_matrix.values).dot(w_vec))\n",
    "    return np.sqrt(var_p)\n",
    "\n",
    "def compute_risk_contributions(\n",
    "    weights: Dict[str, float],\n",
    "    cov_matrix: pd.DataFrame\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Compute each asset’s risk contribution to total portfolio volatility.\n",
    "    RC_i = w_i * (Σ w)_i / σ_p\n",
    "    Returns a Series indexed by ticker.\n",
    "    \"\"\"\n",
    "    w = normalize_weights(weights)\n",
    "    w_vec = np.array([w[t] for t in cov_matrix.index])\n",
    "    sigma_p = compute_portfolio_volatility(weights, cov_matrix)\n",
    "    # marginal contributions = (Σ w)_i\n",
    "    marg = cov_matrix.values.dot(w_vec)\n",
    "    rc = w_vec * marg / sigma_p\n",
    "    return pd.Series(rc, index=cov_matrix.index, name=\"risk_contrib\")\n",
    "\n",
    "def compute_herfindahl(\n",
    "    weights: Dict[str, float]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute the Herfindahl index = sum(w_i^2).\n",
    "    Indicates portfolio concentration (0 = fully diversified, 1 = single asset).\n",
    "    \"\"\"\n",
    "    w = normalize_weights(weights)\n",
    "    return float(sum([w_i ** 2 for w_i in w.values()]))\n",
    "\n",
    "# Example usage snippet (to paste in your notebook):\n",
    "#\n",
    "# import pandas as pd\n",
    "# from portfolio_risk import (\n",
    "#     compute_portfolio_returns,\n",
    "#     compute_covariance_matrix,\n",
    "#     compute_portfolio_volatility,\n",
    "#     compute_risk_contributions,\n",
    "#     compute_herfindahl\n",
    "# )\n",
    "#\n",
    "# # assume df_ret is a DataFrame of monthly returns for your universe\n",
    "# weights = {\"PCTY\": 0.4, \"AAPL\": 0.6}\n",
    "#\n",
    "# # 1) Portfolio returns\n",
    "# port_ret = compute_portfolio_returns(df_ret, weights)\n",
    "#\n",
    "# # 2) Covariance\n",
    "# cov = compute_covariance_matrix(df_ret)\n",
    "#\n",
    "# # 3) Portfolio volatility\n",
    "# vol = compute_portfolio_volatility(weights, cov)\n",
    "# print(\"Portfolio Volatility:\", vol)\n",
    "#\n",
    "# # 4) Risk contributions\n",
    "# rc = compute_risk_contributions(weights, cov)\n",
    "# print(\"Risk Contributions:\\n\", rc)\n",
    "#\n",
    "# # 5) Concentration (Herfindahl)\n",
    "# h = compute_herfindahl(weights)\n",
    "# print(\"Herfindahl Index:\", h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a474ed-79f4-46a8-b41d-972c1e40be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "def compute_portfolio_variance_breakdown(\n",
    "    weights: Dict[str, float],\n",
    "    idio_var_dict: Dict[str, float],\n",
    "    weighted_factor_var: pd.DataFrame,\n",
    "    vol_m: float\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Returns a structured variance decomposition:\n",
    "      - total variance\n",
    "      - idiosyncratic variance + %\n",
    "      - factor variance + %\n",
    "      - per-factor variance + %\n",
    "    \"\"\"\n",
    "    w = pd.Series(weights)\n",
    "    w2 = w.pow(2)\n",
    "\n",
    "    # Idiosyncratic variance (sum of w_i² * σ²_idio_i)\n",
    "    idio_var_series = pd.Series(idio_var_dict).reindex(w.index).fillna(0.0)\n",
    "    idio_var = float((w2 * idio_var_series).sum())\n",
    "\n",
    "    # Factor variance (sum of weighted factor variance matrix)\n",
    "    factor_var_matrix = (\n",
    "        weighted_factor_var\n",
    "        .drop(columns=[\"industry\", \"subindustry\"], errors=\"ignore\")  # REMOVE\n",
    "        .fillna(0.0)\n",
    "    )\n",
    "    \n",
    "    per_factor_var = factor_var_matrix.sum(axis=0)\n",
    "    factor_var = float(per_factor_var.sum())\n",
    "\n",
    "    # Total portfolio variance\n",
    "    port_var = factor_var + idio_var\n",
    "\n",
    "    # % shares\n",
    "    idio_pct   = idio_var   / port_var if port_var else 0.0\n",
    "    factor_pct = factor_var / port_var if port_var else 0.0\n",
    "\n",
    "    # Breakdown of factor variance by factor\n",
    "    per_factor_var = factor_var_matrix.sum(axis=0)\n",
    "    per_factor_pct = per_factor_var / port_var\n",
    "\n",
    "    return {\n",
    "        \"portfolio_variance\":      port_var,\n",
    "        \"idiosyncratic_variance\":  idio_var,\n",
    "        \"idiosyncratic_pct\":       idio_pct,\n",
    "        \"factor_variance\":         factor_var,\n",
    "        \"factor_pct\":              factor_pct,\n",
    "        \"factor_breakdown_var\":    per_factor_var.to_dict(),\n",
    "        \"factor_breakdown_pct\":    per_factor_pct.to_dict()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e78b9e6-60e2-4565-9934-0802d31ba658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from typing import Dict, List, Optional, Any, Union\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from typing import Dict, List, Optional, Any, Union\n",
    "\n",
    "def get_returns_dataframe(\n",
    "    weights: Dict[str, float],\n",
    "    start_date: str,\n",
    "    end_date: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch and compute monthly returns for all tickers in the weights dictionary.\n",
    "\n",
    "    Args:\n",
    "        weights (Dict[str, float]): Portfolio weights (tickers as keys).\n",
    "        start_date (str): Start date in 'YYYY-MM-DD' format.\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Monthly return series for all tickers, aligned and cleaned.\n",
    "    \"\"\"\n",
    "    rets = {}\n",
    "    for t in weights:\n",
    "        prices = fetch_monthly_close(t, start_date=start_date, end_date=end_date)\n",
    "        rets[t] = calc_monthly_returns(prices)\n",
    "    return pd.DataFrame(rets).dropna()\n",
    "\n",
    "def compute_target_allocations(\n",
    "    weights: Dict[str, float],\n",
    "    expected_returns: Optional[Dict[str, float]] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute target allocations based on expected returns and equal weight comparison.\n",
    "\n",
    "    Args:\n",
    "        weights (Dict[str, float]): Current portfolio weights.\n",
    "        expected_returns (Optional[Dict[str, float]]): Expected returns for tickers.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Allocation table with portfolio weight, equal weight, and proportional return targets.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        \"Portfolio Weight\": pd.Series(weights),\n",
    "        \"Equal Weight\":     pd.Series({t: 1/len(weights) for t in weights})\n",
    "    })\n",
    "    if expected_returns:\n",
    "        total = sum(expected_returns.values())\n",
    "        df[\"Prop Target\"] = pd.Series({t: expected_returns[t]/total for t in expected_returns})\n",
    "        df[\"Prop Diff\"]   = df[\"Portfolio Weight\"] - df[\"Prop Target\"]\n",
    "    df[\"Eq Diff\"] = df[\"Portfolio Weight\"] - df[\"Equal Weight\"]\n",
    "    return df\n",
    "    \n",
    "def build_portfolio_view(\n",
    "    weights: Dict[str, float],\n",
    "    start_date: str,\n",
    "    end_date:   str,\n",
    "    expected_returns: Optional[Dict[str, float]] = None,\n",
    "    stock_factor_proxies: Optional[Dict[str, Dict[str, Union[str, List[str]]]]] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Builds a complete portfolio risk profile using historical returns, factor regressions,\n",
    "    and variance decomposition.\n",
    "\n",
    "    Performs:\n",
    "    - Aggregates returns, volatility, and correlation for the portfolio.\n",
    "    - Runs per-stock single-factor regressions to compute betas (market, momentum, value, industry, subindustry).\n",
    "    - Calculates idiosyncratic volatilities and annualized variances.\n",
    "    - Computes per-stock factor volatilities (σ_i,f) and weighted factor variance (w² · β² · σ²).\n",
    "    - Decomposes portfolio variance into idiosyncratic vs factor-driven.\n",
    "    - Aggregates per-industry ETF variance contributions (based on industry proxies).\n",
    "    - Computes portfolio-level factor betas and Herfindahl concentration.\n",
    "    - Summarizes per-industry group betas from weighted contributions of individual stock betas.\n",
    "\n",
    "    Args:\n",
    "        weights (Dict[str, float]):\n",
    "            Portfolio weights by ticker (not required to sum to 1).\n",
    "        start_date (str):\n",
    "            Historical window start date (format: YYYY-MM-DD).\n",
    "        end_date (str):\n",
    "            Historical window end date (format: YYYY-MM-DD).\n",
    "        expected_returns (Optional[Dict[str, float]]):\n",
    "            Optional target returns per ticker for allocation gap display.\n",
    "        stock_factor_proxies (Optional[Dict]):\n",
    "            Mapping of each stock to its factor proxies:\n",
    "                - \"market\": ETF ticker (e.g., SPY)\n",
    "                - \"momentum\": ETF ticker (e.g., MTUM)\n",
    "                - \"value\": ETF ticker (e.g., IWD)\n",
    "                - \"industry\": ETF ticker (e.g., SOXX)\n",
    "                - \"subindustry\": list of tickers (e.g., [\"PAYC\", \"CDAY\"])\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: Portfolio diagnostics including:\n",
    "            - 'allocations': target vs actual vs expected returns\n",
    "            - 'portfolio_returns': aggregated monthly returns\n",
    "            - 'covariance_matrix': asset return covariances\n",
    "            - 'correlation_matrix': asset return correlations\n",
    "            - 'volatility_monthly': annualized volatility from monthly returns\n",
    "            - 'volatility_annual': total annual portfolio volatility\n",
    "            - 'risk_contributions': risk contribution by asset\n",
    "            - 'herfindahl': portfolio concentration score\n",
    "            - 'df_stock_betas': per-stock factor betas from regressions\n",
    "            - 'portfolio_factor_betas': weighted sum of factor exposures\n",
    "            - 'factor_vols': per-stock annualized factor volatilities\n",
    "            - 'weighted_factor_var': w² · β² · σ² contributions\n",
    "            - 'asset_vol_summary': asset-level volatility and idio stats\n",
    "            - 'variance_decomposition': total vs idio vs factor variance\n",
    "            - 'industry_variance': {\n",
    "                'absolute': variance by industry proxy,\n",
    "                'percent_of_portfolio': % variance per industry,\n",
    "                'per_industry_group_beta': weighted betas per industry ETF\n",
    "              }\n",
    "    \"\"\"\n",
    "    # ─── 0. Portfolio Return Setup ──────────────────────────────────────────────\n",
    "    df_ret   = get_returns_dataframe(weights, start_date, end_date)\n",
    "    df_alloc = compute_target_allocations(weights, expected_returns)\n",
    "\n",
    "    port_ret = compute_portfolio_returns(df_ret, weights)\n",
    "    cov_mat  = compute_covariance_matrix(df_ret)\n",
    "    corr_mat = compute_correlation_matrix(df_ret)\n",
    "\n",
    "    vol_m = compute_portfolio_volatility(weights, cov_mat)\n",
    "    vol_a = vol_m * np.sqrt(12)\n",
    "    rc    = compute_risk_contributions(weights, cov_mat)\n",
    "    hhi   = compute_herfindahl(weights)\n",
    "\n",
    "    w_series                = pd.Series(weights)\n",
    "\n",
    "    # ─── 1. Stock-Level Factor Exposures ────────────────────────────────────────\n",
    "    df_stock_betas = pd.DataFrame(index=weights.keys())\n",
    "    idio_var_dict  = {}\n",
    "\n",
    "    if stock_factor_proxies:\n",
    "        for ticker, proxies in stock_factor_proxies.items():\n",
    "            # Fetch stock returns\n",
    "            prices    = fetch_monthly_close(ticker, start_date=start_date, end_date=end_date)\n",
    "            stock_ret = calc_monthly_returns(prices)\n",
    "            idx       = stock_ret.index\n",
    "\n",
    "            # Build aligned factor series\n",
    "            fac_dict: Dict[str, pd.Series] = {}\n",
    "\n",
    "            mkt_t = proxies.get(\"market\")\n",
    "            if mkt_t:\n",
    "                mkt_ret = calc_monthly_returns(\n",
    "                    fetch_monthly_close(mkt_t, start_date=start_date, end_date=end_date)\n",
    "                ).reindex(idx).dropna()\n",
    "                fac_dict[\"market\"] = mkt_ret\n",
    "\n",
    "            mom_t = proxies.get(\"momentum\")\n",
    "            if mom_t and mkt_t:\n",
    "                mom_ret = fetch_excess_return(mom_t, mkt_t, start_date, end_date).reindex(idx).dropna()\n",
    "                fac_dict[\"momentum\"] = mom_ret\n",
    "\n",
    "            val_t = proxies.get(\"value\")\n",
    "            if val_t and mkt_t:\n",
    "                val_ret = fetch_excess_return(val_t, mkt_t, start_date, end_date).reindex(idx).dropna()\n",
    "                fac_dict[\"value\"] = val_ret\n",
    "\n",
    "            for facname in (\"industry\", \"subindustry\"):\n",
    "                proxy = proxies.get(facname)\n",
    "                if proxy:\n",
    "                    if isinstance(proxy, list):\n",
    "                        ser = fetch_peer_median_monthly_returns(proxy, start_date, end_date)\n",
    "                    else:\n",
    "                        ser = calc_monthly_returns(\n",
    "                            fetch_monthly_close(proxy, start_date=start_date, end_date=end_date)\n",
    "                        )\n",
    "                    fac_dict[facname] = ser.reindex(idx).dropna()\n",
    "\n",
    "            # drop rows with any NaN\n",
    "            factor_df  = pd.DataFrame(fac_dict).dropna(how=\"any\")\n",
    "            if factor_df.empty:\n",
    "                continue # Skip if no usable data\n",
    "        \n",
    "            aligned_s = stock_ret.reindex(factor_df.index)\n",
    "                    \n",
    "            # Run single-factor regression to get betas\n",
    "            betas = compute_stock_factor_betas(\n",
    "                aligned_s,                               # stock on same dates\n",
    "                {c: factor_df[c] for c in factor_df}     # factors on same dates\n",
    "            )\n",
    "            df_stock_betas.loc[ticker, betas.keys()] = pd.Series(betas)\n",
    "\n",
    "            # Idiosyncratic variance (monthly → annual)\n",
    "            X      = sm.add_constant(factor_df)\n",
    "            resid  = aligned_s - sm.OLS(aligned_s, X).fit().fittedvalues\n",
    "        \n",
    "            # Convert monthly residual variance to annual variance\n",
    "            monthly_idio_var = resid.var(ddof=1)\n",
    "            annual_idio_var = monthly_idio_var * 12\n",
    "            idio_var_dict[ticker] = float(annual_idio_var)\n",
    "\n",
    "    # ─── 2. Compute Factor Volatility & Weighted Variance ───────────────────────\n",
    "    df_factor_vols   = pd.DataFrame(index=df_stock_betas.index,\n",
    "                                    columns=df_stock_betas.columns)   # σ_i,f (annual)\n",
    "    weighted_factor_var = pd.DataFrame(index=df_stock_betas.index,\n",
    "                                       columns=df_stock_betas.columns) # w_i² β² σ²\n",
    "    \n",
    "    if stock_factor_proxies:                                           # ← guard\n",
    "        w2 = pd.Series(weights).pow(2)                                 # w_i²\n",
    "    \n",
    "        for tkr, proxies in stock_factor_proxies.items():\n",
    "    \n",
    "            # ----- rebuild this stock’s factor-return dict (same logic as above) --\n",
    "            idx_stock = calc_monthly_returns(\n",
    "                fetch_monthly_close(tkr, start_date, end_date)\n",
    "            ).index\n",
    "            fac_ret: Dict[str, pd.Series] = {}\n",
    "    \n",
    "            mkt = proxies.get(\"market\")\n",
    "            if mkt:\n",
    "                fac_ret[\"market\"] = calc_monthly_returns(\n",
    "                    fetch_monthly_close(mkt, start_date, end_date)\n",
    "                ).reindex(idx_stock).dropna()\n",
    "    \n",
    "            def _excess(etf: str) -> pd.Series:\n",
    "                return fetch_excess_return(etf, mkt, start_date, end_date\n",
    "                       ).reindex(idx_stock).dropna()\n",
    "    \n",
    "            if proxies.get(\"momentum\"):\n",
    "                fac_ret[\"momentum\"] = _excess(proxies[\"momentum\"])\n",
    "            if proxies.get(\"value\"):\n",
    "                fac_ret[\"value\"]    = _excess(proxies[\"value\"])\n",
    "    \n",
    "            for fac in (\"industry\", \"subindustry\"):\n",
    "                proxy = proxies.get(fac)\n",
    "                if proxy:\n",
    "                    ser = ( fetch_peer_median_monthly_returns(proxy, start_date, end_date)\n",
    "                            if isinstance(proxy, list)\n",
    "                            else calc_monthly_returns(\n",
    "                                    fetch_monthly_close(proxy, start_date, end_date) ) )\n",
    "                    fac_ret[fac] = ser.reindex(idx_stock).dropna()\n",
    "    \n",
    "            if not fac_ret:         # nothing to measure\n",
    "                continue\n",
    "    \n",
    "            # ----- annual σ_i,f ----------------------------------------------------\n",
    "            sigmas = pd.Series({f: r.std(ddof=1) * np.sqrt(12) for f, r in fac_ret.items()})\n",
    "            df_factor_vols.loc[tkr, sigmas.index] = sigmas\n",
    "    \n",
    "            # df_factor_vols  : σ-table (annual factor vols by stock)\n",
    "            df_factor_vols = (\n",
    "                df_factor_vols\n",
    "                    .apply(pd.to_numeric, errors=\"coerce\")  # force numeric, NaNs where bad\n",
    "                    .astype(\"float64\", copy=False)          # ensure float dtype, no copy if already\n",
    "                    .fillna(0.0)                           # now safe – no warning\n",
    "            )\n",
    "            \n",
    "            # betas_filled   : β-table with NaNs → 0.0\n",
    "            betas_filled = (\n",
    "                df_stock_betas\n",
    "                    .apply(pd.to_numeric, errors=\"coerce\")\n",
    "                    .astype(\"float64\", copy=False)\n",
    "                    .fillna(0.0)\n",
    "            )\n",
    "    \n",
    "        # ----- weighted factor variance  w_i² β_i,f² σ_i,f² -----------------------\n",
    "        weighted_factor_var = betas_filled.pow(2) * df_factor_vols.pow(2)\n",
    "        weighted_factor_var = weighted_factor_var.mul(w2, axis=0)\n",
    "\n",
    "\n",
    "    # ─── 3a. Aggregate Industry-Level Variance ───────────────────────────────────\n",
    "    industry_var_dict = {}\n",
    "    \n",
    "    # Step: reverse-map which stock maps to which industry ETF\n",
    "    for tkr, proxies in stock_factor_proxies.items():\n",
    "        ind = proxies.get(\"industry\")\n",
    "        if ind:\n",
    "            v = weighted_factor_var.loc[tkr, \"industry\"] if \"industry\" in weighted_factor_var.columns else 0.0\n",
    "            industry_var_dict[ind] = industry_var_dict.get(ind, 0.0) + v\n",
    "\n",
    "    # ─── 3b. Compute Per-Industry Group Beta (and max weighted exposure) ──────────────\n",
    "    industry_groups: Dict[str, float] = {}\n",
    "\n",
    "    for ticker in w_series.index:\n",
    "        proxy = stock_factor_proxies.get(ticker, {}).get(\"industry\")\n",
    "        beta = df_stock_betas.get(\"industry\", {}).get(ticker, 0.0)\n",
    "        weight = w_series[ticker]\n",
    "        if proxy:\n",
    "            industry_groups[proxy] = industry_groups.get(proxy, 0.0) + (weight * beta)\n",
    "    \n",
    "    # ─── 4. Final Portfolio Stats (Volatility, Idio, Betas) ─────────────────────\n",
    "    portfolio_factor_betas  = df_stock_betas.mul(w_series, axis=0).sum(skipna=True)\n",
    "\n",
    "    # 4a) per-asset annualised stats ----------------------------------------\n",
    "    asset_vol_a = df_ret.std(ddof=1) * np.sqrt(12)               # total σ_annual\n",
    "    asset_var_m = df_ret.var(ddof=1)                             # monthly σ²\n",
    "    w_series    = pd.Series(weights)\n",
    "    \n",
    "    # idiosyncratic\n",
    "    idio_var_a  = pd.Series(idio_var_dict).reindex(w_series.index)         # already annual\n",
    "    idio_vol_a  = idio_var_a.pow(0.5)                                       # √(annual var)\n",
    "    weighted_idio_var_model = w_series.pow(2) * idio_var_a  # w² · σ²_idio\n",
    "\n",
    "    # Manually compute (w × σ_idio)² for comparison\n",
    "    weighted_idio_vol = idio_vol_a * w_series\n",
    "    weighted_idio_var_manual = (weighted_idio_vol) ** 2\n",
    "    \n",
    "    df_asset = pd.DataFrame({\n",
    "        \"Vol A\":              asset_vol_a,                       # total annual σ\n",
    "        \"Weighted Vol A\":     asset_vol_a * w_series,\n",
    "        #\"Var M\":              asset_var_m,                       # monthly total σ² (for reference)\n",
    "        #\"Weighted Var M\":     asset_var_m * (w_series ** 2),\n",
    "        \"Idio Vol A\":         idio_vol_a,                        # idio annual σ\n",
    "        \"Weighted Idio Vol A\": weighted_idio_vol,\n",
    "        \"Weighted Idio Var\": weighted_idio_var_model,\n",
    "        #\"Manual Weighted Idio Var\": weighted_idio_var_manual\n",
    "        #\"Weighted IdioVar A\": idio_var_a * (w_series ** 2),\n",
    "    })\n",
    "\n",
    "    # ─── 5. Industry Variance % Contribution ────────────────────────────────────\n",
    "    total_port_var = (\n",
    "        compute_portfolio_variance_breakdown(\n",
    "            weights, idio_var_dict, weighted_factor_var, vol_m\n",
    "        )[\"portfolio_variance\"]\n",
    "    )\n",
    "    \n",
    "    industry_pct_dict = {\n",
    "        k: v / total_port_var if total_port_var else 0.0\n",
    "        for k, v in industry_var_dict.items()\n",
    "    }\n",
    "\n",
    "    # ─── 6. Assemble Final Output ───────────────────────────────────────────────\n",
    "    return {\n",
    "        \"allocations\":            df_alloc,\n",
    "        \"covariance_matrix\":      cov_mat,\n",
    "        \"correlation_matrix\":     corr_mat,\n",
    "        \"volatility_monthly\":     vol_m,\n",
    "        \"volatility_annual\":      vol_a,\n",
    "        \"risk_contributions\":     rc,\n",
    "        \"herfindahl\":             hhi,\n",
    "        \"df_stock_betas\":         df_stock_betas,\n",
    "        \"portfolio_factor_betas\": portfolio_factor_betas,\n",
    "        \"factor_vols\":            df_factor_vols,         \n",
    "        \"weighted_factor_var\":    weighted_factor_var, \n",
    "        \"asset_vol_summary\":      df_asset,\n",
    "        \"portfolio_returns\":      port_ret,\n",
    "        \"variance_decomposition\": compute_portfolio_variance_breakdown(\n",
    "        weights, idio_var_dict, weighted_factor_var, vol_m),\n",
    "        \"industry_variance\": {\n",
    "        \"absolute\": industry_var_dict,\n",
    "        \"percent_of_portfolio\": industry_pct_dict,\n",
    "        \"per_industry_group_beta\": industry_groups,\n",
    "    }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be8431e-1047-45b5-9d8b-971127bea392",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
